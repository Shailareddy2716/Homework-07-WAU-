# -*- coding: utf-8 -*-
"""Homework-07-Dag-01

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Q-lr9QIWqACqKQ4BHw30Ze-vkcDevOM
"""

from airflow import DAG
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from airflow.decorators import task
from airflow.utils.dates import days_ago
from datetime import datetime
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 10, 2),
    'retries': 1,
}

with DAG(dag_id='import_two_table_to_Snowflake',
         default_args=default_args,
         schedule_interval='30 2 * * *',
         catchup=False) as dag:

    @task
    def create_stage():
        snowflake_hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')
        snowflake_hook.run("""
            CREATE OR REPLACE STAGE dev.raw_data.blob_stage
            url = 's3://s3-geospatial/readonly/'
            file_format = (type = csv, skip_header = 1, field_optionally_enclosed_by = '"');
        """)

    @task
    def load_user_session_channel():
        snowflake_hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')
        snowflake_hook.run("""
            COPY INTO dev.raw_data.user_session_channel
            FROM @dev.raw_data.blob_stage/user_session_channel.csv;
        """)

    @task
    def load_session_timestamp():
        snowflake_hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')
        snowflake_hook.run("""
            COPY INTO dev.raw_data.session_timestamp
            FROM @dev.raw_data.blob_stage/session_timestamp.csv;
        """)

    # Order of tasks to be executed
    create_stage() >> load_user_session_channel() >> load_session_timestamp()